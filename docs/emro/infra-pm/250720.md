
## 작업내용
- EKS 버전 업그레이드 (1.31 -> 1.33)
- EKS Node OS 업그레이드 (AL2 -> AL2023)
	- 노드그룹 재 생성
- 인프라 노드그룹 scale in (2 -> 1)
	- gitlab, jenkins replicas 0

### Step 0 사전작업 및 작업 중 확인사항
- 노드그룹 생성
- kafka replica 0 설정
~~~
kubectl scale sts kafka -n kafka --replicas=0
~~~
- 업그레이드 시 budget 걸리면 해당 pod restart 적용
~~~
kubectl rollout restart deployment ebs-csi-controller -n kube-system


   kubectl rollout restart daemonset npki -n kube-system
 
~~~


#### 노드그룹 scale in
- gitlab, jenkins replica 0 설정
~~~
kubectl scale deploy gitlab -n gitlab --replicas=0
kubectl scale sts jenkins -n jenkins --replicas=0
~~~
- 콘솔에서 노드 scale in 설정
- 노드 메모리 확인
~~~
kubectl top node
~~~


### Step2
#### OS 업그레이드
- 노드그룹 생성 (AL2023) (사전에 생성)
- node drain & cordon
~~~
kubectl cordon "ip-10-0-2-240.ap-northeast-2.compute.internal"
kubectl drain "ip-10-0-2-240.ap-northeast-2.compute.internal" --ignore-daemonsets --delete-emptydir-data --force



kubectl get pod -A -o wide | grep ip-10-0-2-240.ap-northeast-2.compute.internal
 kubectl get pod -A -o wide | grep ip-10-0-2-201.ap-northeast-2.compute.internal


kubectl rollout restart deployment ebs-csi-controller -n kube-system








kubectl get pod -A -o wide | grep ip-10-0-2-252.ap-northeast-2.compute.internal

kubectl cordon "ip-10-0-2-252.ap-northeast-2.compute.internal"
kubectl drain "ip-10-0-2-252.ap-northeast-2.compute.internal" --ignore-daemonsets --delete-emptydir-data --force



 kubectl get pod -A -o wide | grep ip-10-0-3-71.ap-northeast-2.compute.internal

kubectl cordon "ip-10-0-3-71.ap-northeast-2.compute.internal"
kubectl drain "ip-10-0-3-71.ap-northeast-2.compute.internal" --ignore-daemonsets --delete-emptydir-data --force



 kubectl get pod -A -o wide | grep ip-10-0-2-96.ap-northeast-2.compute.internal
 kubectl get pod -A -o wide | grep ip-10-0-3-113.ap-northeast-2.compute.internal



image: docker.io/bitnami/kafka:3.2.0-debian-11-r12


kubectl edit sts kafka -n kafka
image: docker.io/bitnami/bitnami-shell:11-debian-11-r12



kubectl cordon "ip-10-0-2-150.ap-northeast-2.compute.internal"
kubectl drain "ip-10-0-2-150.ap-northeast-2.compute.internal" --ignore-daemonsets --delete-emptydir-data --force


~~~
- 기존 노드 (AL2) Scale in 0


#### control plane 업그레이드
- 1.31 ->  1.32
- 1.32 -> 1.33

#### Worker Node 업그레이드
- 1.31 -> 1.33

#### add on 업데이트
- latest 로 업데이트

### Step 4
- kafka replica 1 설정
~~~
kubectl scale sts kafka -n kafka --replicas=1
~~~


### 확인사항

- pod 구동 확인
- node ssh 접속 확인
- 웹서비스 접속 확인
- NPKI 생성 확인 (없으면 npki 데몬 재실행)
- 데이터독 EKS 정상 관제 확인
- PV, PVC 확인
- 카프카 동작 확인
- uptime 동작 확인 (pv)
- 인프라 노드 메모리 사용량 확인.


