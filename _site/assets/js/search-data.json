{"0": {
    "doc": "pod í…ŒìŠ¤íŠ¸í•˜ê¸°",
    "title": "pod í…ŒìŠ¤íŠ¸í•˜ê¸°",
    "content": "ë…¸ë“œì— ë°°í¬ëœ ì»¨í…Œì´ë„ˆ ì •ë³´ í™•ì¸ Containerd clients 3ì¢… : ctr, nerdctl, crictl . (Administrator@myeks:N/A) [root@myeks-host ~]# ssh ec2-user@$N1 ctr --version ctr github.com/containerd/containerd 1.7.25 (Administrator@myeks:N/A) [root@myeks-host ~]# ssh ec2-user@$N1 ctr NAME: ctr - __ _____/ /______ / ___/ __/ ___/ / /__/ /_/ / \\___/\\__/_/ containerd CLI # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ í™•ì¸ ssh ec2-user@$N1 sudo ctr ns list NAME LABELS k8s.io # ì»¨í…Œì´ë„ˆ ë¦¬ìŠ¤íŠ¸ í™•ì¸ for i in $N1 $N2; do echo \"&gt;&gt; node $i &lt;&lt;\"; ssh ec2-user@$i sudo ctr -n k8s.io container list; echo; done CONTAINER IMAGE RUNTIME 28b6a15c475e32cd8777c1963ba684745573d0b6053f80d2d37add0ae841eb45 602401143452.dkr.ecr-fips.us-east-1.amazonaws.com/eks/pause:3.5 io.containerd.runc.v2 4f266ebcee45b133c527df96499e01ec0c020ea72785eb10ef63b20b5826cf7c 602401143452.dkr.ecr-fips.us-east-1.amazonaws.com/eks/pause:3.5 io.containerd.runc.v2 ... # ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ í™•ì¸ for i in $N1 $N2; do echo \"&gt;&gt; node $i &lt;&lt;\"; ssh ec2-user@$i sudo ctr -n k8s.io image list --quiet; echo; done ... # íƒœìŠ¤í¬ ë¦¬ìŠ¤íŠ¸ í™•ì¸ for i in $N1 $N2; do echo \"&gt;&gt; node $i &lt;&lt;\"; ssh ec2-user@$i sudo ctr -n k8s.io task list; echo; done ... ## ì˜ˆì‹œ) ê° í…ŒìŠ¤í¬ì˜ PID(3706) í™•ì¸ ssh ec2-user@$N1 sudo ps -c 3706 PID CLS PRI TTY STAT TIME COMMAND 3099 TS 19 ? Ssl 0:01 kube-proxy --v=2 --config=/var/lib/kube-proxy-config/config --hostname-override=ip-192-168-1-229.ap-northeast-2.compute.internal . ECR í¼ë¸”ë¦­ Repository ì‚¬ìš© : í¼ë¸”ë¦­ Repo ëŠ” ì„¤ì • ì‹œ us-east-1 ë¥¼ ì‚¬ìš© . # í¼ë¸”ë¦­ ECR ì¸ì¦ aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws cat /root/.docker/config.json | jq # í¼ë¸”ë¦­ Repo ê¸°ë³¸ ì •ë³´ í™•ì¸ aws ecr-public describe-registries --region us-east-1 | jq { \"registries\": [ { \"registryId\": \"738612635754\", \"registryArn\": \"arn:aws:ecr-public::738612635754:registry/738612635754\", \"registryUri\": \"\", \"verified\": false, \"aliases\": [] } ] } # í¼ë¸”ë¦­ Repo ìƒì„± NICKNAME=seongtki aws ecr-public create-repository --repository-name $NICKNAME/nginx --region us-east-1 # ìƒì„±ëœ í¼ë¸”ë¦­ Repo í™•ì¸ aws ecr-public describe-repositories --region us-east-1 | jq REPOURI=$(aws ecr-public describe-repositories --region us-east-1 | jq -r .repositories[].repositoryUri) echo $REPOURI public.ecr.aws/o2n8a7w3/seongtki/nginx # ì´ë¯¸ì§€ íƒœê·¸ docker pull nginx:alpine docker images docker tag nginx:alpine $REPOURI:latest docker images # ì´ë¯¸ì§€ ì—…ë¡œë“œ docker push $REPOURI:latest # íŒŒë“œ ì‹¤í–‰ kubectl run mynginx --image $REPOURI kubectl get pod NAME READY STATUS RESTARTS AGE mario-6d8c76fd8d-2mx7k 1/1 Running 0 10m mynginx 0/1 ContainerCreating 0 3s kubectl delete pod mynginx ****# í¼ë¸”ë¦­ ì´ë¯¸ì§€ ì‚­ì œ aws ecr-public batch-delete-image \\ --repository-name $NICKNAME/nginx \\ --image-ids imageTag=latest \\ --region us-east-1 # í¼ë¸”ë¦­ Repo ì‚­ì œ aws ecr-public delete-repository --repository-name $NICKNAME/nginx --force --region us-east-1 . ì„œë¹„ìŠ¤/ë””í”Œë¡œì´ë¨¼íŠ¸(mario ê²Œì„) ë°°í¬ í…ŒìŠ¤íŠ¸ with AWS CLB . # í„°ë¯¸ë„1 (ëª¨ë‹ˆí„°ë§) watch -d 'kubectl get pod,svc' # ìˆ˜í¼ë§ˆë¦¬ì˜¤ ë””í”Œë¡œì´ë¨¼íŠ¸ ë°°í¬ cat &lt;&lt;EOT &gt; mario.yaml apiVersion: apps/v1 kind: Deployment metadata: name: mario labels: app: mario spec: replicas: 1 selector: matchLabels: app: mario template: metadata: labels: app: mario spec: containers: - name: mario image: pengbai/docker-supermario --- apiVersion: v1 kind: Service metadata: name: mario spec: selector: app: mario ports: - port: 80 protocol: TCP targetPort: 8080 type: LoadBalancer EOT kubectl apply -f mario.yaml # ë°°í¬ í™•ì¸ : CLB ë°°í¬ í™•ì¸ kubectl get deploy,svc,ep mario # ë§ˆë¦¬ì˜¤ ê²Œì„ ì ‘ì† : CLB ì£¼ì†Œë¡œ ì›¹ ì ‘ì† kubectl get svc mario -o jsonpath={.status.loadBalancer.ingress[0].hostname} | awk '{ print \"Maria URL = http://\"$1 }' (Administrator@myeks:N/A) [root@myeks-host ~]# kubectl get svc mario -o jsonpath={.status.loadBalancer.ingress[0].hostname} | awk '{ print \"Maria URL = http://\"$1 }' Maria URL = http://a3fbbb85ed6df4e0288fd3bf4944cfc3-734390217.ap-northeast-2.elb.amazonaws.com . ",
    "url": "/docs/eks-hands-on/part01/application/",
    
    "relUrl": "/docs/eks-hands-on/part01/application/"
  },"1": {
    "doc": "eks ì„¤ì¹˜, public & privaet ëª¨ë“œ",
    "title": "AWS API ì„œë²„ ì—”ë“œí¬ì¸íŠ¸ ì—‘ì„¸ìŠ¤",
    "content": ". public . while true; do dig +short $APIDNS ; echo \"------------------------------\" ; date; sleep 1; done ------------------------------ 2025. 02. 09. (ì¼) 01:40:42 KST 15.164.132.1 3.38.114.154 . kubeletê³¼ kube-proxyì˜ ì—”ë“œí¬ì¸íŠ¸ëŠ” ê³µì¸ì•„ì´í”¼ë¡œ ë˜ì–´ìˆë‹¤. (ì¦‰, ì»¨íŠ¸ë¡¤ í”Œë ˆì¸ê³¼ ë…¸ë“œ í†µì‹ (kubelet, kube-proxy)ì´ ì¼ì–´ë‚  ë•Œ í•­ìƒ ê³µì¸ì•„ì´í”¼ë¡œ ì†¡ìˆ˜ì‹ ëœë‹¤ëŠ” ê²ƒ.) . while true; do ssh ec2-user@$N1 sudo ss -tnp | egrep 'kubelet|kube-proxy' ; echo ; ssh ec2-user@$N2 sudo ss -tnp | egrep 'kubelet|kube-proxy' ; echo \"------------------------------\" ; date; sleep 1; done 2025. 02. 09. (ì¼) 01:40:59 KST ESTAB 0 0 192.168.1.170:36268 15.164.132.1:443 users:((\"kubelet\",pid=2894,fd=26)) ESTAB 0 0 192.168.1.170:55092 3.38.114.154:443 users:((\"kube-proxy\",pid=3113,fd=9)) ESTAB 0 0 [::ffff:192.168.1.170]:10250 [::ffff:192.168.2.122]:47200 users:((\"kubelet\",pid=2894,fd=12)) ESTAB 0 0 [::ffff:192.168.1.170]:10250 [::ffff:192.168.2.156]:52132 users:((\"kubelet\",pid=2894,fd=20)) ESTAB 0 0 192.168.2.244:34406 3.38.114.154:443 users:((\"kube-proxy\",pid=3114,fd=9)) ESTAB 0 0 192.168.2.244:53726 15.164.132.1:443 users:((\"kubelet\",pid=2892,fd=26)) ESTAB 0 0 [::ffff:192.168.2.244]:10250 [::ffff:192.168.2.156]:37720 users:((\"kubelet\",pid=2892,fd=24)) ESTAB 0 0 [::ffff:192.168.2.244]:10250 [::ffff:192.168.2.122]:48800 users:((\"kubelet\",pid=2892,fd=12)) ------------------------------ . public &amp; private . aws eks update-cluster-config --region $AWS_DEFAULT_REGION --name $CLUSTER_NAME --resources-vpc-config endpointPublicAccess=true,publicAccessCidrs=\"$(curl -s ipinfo.io/ip)/32\",endpointPrivateAccess=true { \"update\": { \"id\": \"7e6c1d1e-0020-34a9-90f7-645628f32752\", \"status\": \"InProgress\", \"type\": \"EndpointAccessUpdate\", \"params\": [ { \"type\": \"EndpointPublicAccess\", \"value\": \"true\" }, { \"type\": \"EndpointPrivateAccess\", \"value\": \"true\" }, { \"type\": \"PublicAccessCidrs\", \"value\": \"[\\\"13.125.166.72/32\\\"]\" } ], \"createdAt\": \"2025-02-09T01:40:33.081000+09:00\", \"errors\": [] } } . while true; do dig +short $APIDNS ; echo \"------------------------------\" ; date; sleep 1; done ------------------------------ 2025. 02. 09. (ì¼) 01:40:42 KST 15.164.132.1 3.38.114.154 . kubeletê³¼ kube-proxyì˜ ì—”ë“œí¬ì¸íŠ¸ëŠ” ê³µì¸ì•„ì´í”¼ë¡œ ë˜ì–´ìˆë‹¤. (ì¦‰, ì»¨íŠ¸ë¡¤ í”Œë ˆì¸ê³¼ ë…¸ë“œ í†µì‹ (kubelet, kube-proxy)ì´ ì¼ì–´ë‚  ë•Œ í•­ìƒ ê³µì¸ì•„ì´í”¼ë¡œ ì†¡ìˆ˜ì‹ ëœë‹¤ëŠ” ê²ƒ.) . while true; do ssh ec2-user@$N1 sudo ss -tnp | egrep 'kubelet|kube-proxy' ; echo ; ssh ec2-user@$N2 sudo ss -tnp | egrep 'kubelet|kube-proxy' ; echo \"------------------------------\" ; date; sleep 1; done 2025. 02. 09. (ì¼) 01:40:59 KST ESTAB 0 0 192.168.1.170:36268 15.164.132.1:443 users:((\"kubelet\",pid=2894,fd=26)) ESTAB 0 0 192.168.1.170:55092 3.38.114.154:443 users:((\"kube-proxy\",pid=3113,fd=9)) ESTAB 0 0 [::ffff:192.168.1.170]:10250 [::ffff:192.168.2.122]:47200 users:((\"kubelet\",pid=2894,fd=12)) ESTAB 0 0 [::ffff:192.168.1.170]:10250 [::ffff:192.168.2.156]:52132 users:((\"kubelet\",pid=2894,fd=20)) ESTAB 0 0 192.168.2.244:34406 3.38.114.154:443 users:((\"kube-proxy\",pid=3114,fd=9)) ESTAB 0 0 192.168.2.244:53726 15.164.132.1:443 users:((\"kubelet\",pid=2892,fd=26)) ESTAB 0 0 [::ffff:192.168.2.244]:10250 [::ffff:192.168.2.156]:37720 users:((\"kubelet\",pid=2892,fd=24)) ESTAB 0 0 [::ffff:192.168.2.244]:10250 [::ffff:192.168.2.122]:48800 users:((\"kubelet\",pid=2892,fd=12)) ------------------------------ . ìœ„ ê²°ê³¼ public &amp; private ìœ¼ë¡œ ë³€ê²½ë˜ëŠ”ë°, ë² ìŠ¤ì²œì—ì„œ kubctl ëª…ë ¹ì–´ê°€ ë”ì´ìƒ ì‹¤í–‰ë˜ì§€ ì•ŠëŠ”ë‹¤. ì™œëƒí•˜ë©´ EKS owned ENI í˜•íƒœë¡œ ê´€ë¦¬ë˜ëŠ” ENIì˜ ì‹œíë¦¬í‹°ê·¸ë£¹ì— ë² ìŠ¤ì²œì— ëŒ€í•œ ì¸ë°”ìš´ë“œ ì„¤ì •ì„ í•´ë†“ì§€ ì•Šì•˜ê¸°ì— í†µì‹ ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤. (ì•„ë˜ì™€ ê°™ì´ ì²˜ë¦¬í•˜ë©´ ë¨) . ControlPlaneSecurityGroupì„ ì°¾ê³ , ë² ìŠ¤ì²œ Ip ì¸ë°”ìš´ë“œë¥¼ Exposeí•œë‹¤ . CPSGID=$(aws ec2 describe-security-groups --filters Name=group-name,Values=*ControlPlaneSecurityGroup* --query \"SecurityGroups[*].[GroupId]\" --output text) . aws ec2 authorize-security-group-ingress --group-id $CPSGID --protocol '-1' --cidr 192.168.1.100/32 { \"Return\": true, \"SecurityGroupRules\": [ { \"SecurityGroupRuleId\": \"sgr-0e61ded354b5415b0\", \"GroupId\": \"sg-0e301754ec8ac3212\", \"GroupOwnerId\": \"738612635754\", \"IsEgress\": false, \"IpProtocol\": \"-1\", \"FromPort\": -1, \"ToPort\": -1, \"CidrIpv4\": \"192.168.1.100/32\", \"SecurityGroupRuleArn\": \"arn:aws:ec2:ap-northeast-2:738612635754:security-group-rule/sgr-0e61ded354b5415b0\" } ] } . | dig +short $APIDNS | dig ê²°ê³¼ê°€ ì»¨íŠ¸ë¡¤ í”Œë ˆì¸ì—ì„œ ê´€ë¦¬í•˜ëŠ” ì‚¬ì„¤ì•„ì´í”¼ê°€ ì¡°íšŒë˜ê¸° ì‹œì‘í•œë‹¤. | ì´ëŠ”, ë°ì´í„°í”Œë ˆì¸ê³¼ì˜ í†µì‹ ì´ aws ë‚´ë¶€ì ìœ¼ë¡œ ì¼ì–´ë‚œë‹¤ëŠ” ì˜ë¯¸ | . ------------------------------ 2025. 02. 09. (ì¼) 01:56:05 KST 192.168.1.152 192.168.2.82 . | node dns ì—­ì‹œ internalë¡œ ë³€ê²½ë¨ | . kubectl get node -v=6 I0209 01:58:11.886004 18989 loader.go:395] Config loaded from file: /root/.kube/config I0209 01:58:12.857831 18989 round_trippers.go:553] GET https://4049728AFEB8F9CEA17AA2E19BD71657.gr7.ap-northeast-2.eks.amazonaws.com/api/v1/nodes?limit=500 200 OK in 961 milliseconds NAME STATUS ROLES AGE VERSION ip-192-168-1-170.ap-northeast-2.compute.internal Ready &lt;none&gt; 99m v1.31.4-eks-aeac579 ip-192-168-2-244.ap-northeast-2.compute.internal Ready &lt;none&gt; 99m v1.31.4-eks-aeac579 . | ì‹¤ì œ í†µì‹ ì€, ì•„ë˜ì™€ê°™ì´ ì¬ì‹¤í–‰ ì´í›„ ì ìš©ë¨ì„ í™•ì¸ | . # ëª¨ë‹ˆí„°ë§ : tcp peer ì •ë³´ ë³€í™” í™•ì¸ while true; do ssh ec2-user@$N1 sudo ss -tnp | egrep 'kubelet|kube-proxy' ; echo ; ssh ec2-user@$N2 sudo ss -tnp | egrep 'kubelet|kube-proxy' ; echo \"------------------------------\" ; date; sleep 1; done # kube-proxy rollout kubectl rollout restart ds/kube-proxy -n kube-system . while true; do ssh ec2-user@$N1 sudo ss -tnp | egrep 'kubelet|kube-proxy' ; echo ; ssh ec2-user@$N2 sudo ss -tnp | egrep 'kubelet|kube-proxy' ; echo \"------------------------------\" ; date; sleep 1; done ESTAB 0 0 192.168.1.170:44346 192.168.2.82:443 users:((\"kubelet\",pid=60428,fd=9)) ESTAB 0 0 192.168.1.170:57082 192.168.1.152:443 users:((\"kube-proxy\",pid=60264,fd=9)) ESTAB 0 0 192.168.2.244:51864 192.168.1.152:443 users:((\"kube-proxy\",pid=59674,fd=9)) ESTAB 0 0 192.168.2.244:54588 192.168.2.82:443 users:((\"kubelet\",pid=59859,fd=12)) . í—ˆìš©ì•„ì´í”¼ë¥¼ ì‚­ì œí•˜ë©´ ? . | ë‹¹ì—°íˆ kubectl api í†µì‹ ì€ ëœë‹¤. ì• ì´ˆì— ë°ì´í„°í”Œë ˆì¸ ì´ ìˆëŠ” ì„œë¸Œë„·ì— ì†í•´ìˆê¸° ë•Œë¬¸ì— EKS owned ENI ì„ í†µí•´ í†µì‹ í•˜ê³  ìˆì—ˆê¸°ì—, í—ˆìš©ì•„ì´í”¼ì™€ëŠ” ë¬´ê´€í•˜ë‹¤. (private ëª¨ë“œì—ì„œ í†µì‹  ê°€ëŠ¥) . | ë‹¤ë¥¸ ì‘ì—…ê³µê°„ì—ì„œ kubectl í†µì‹ ì´ í•„ìš”í•˜ë‹¤ë©´, aws ìê²©ì¦ëª… í›„ì— aws eks update-kubeconfig --region ap-northeast-2 --name myeks ì™€ ê°™ì´ eks ì ‘ê·¼ì •ë³´ë¥¼ ì‘ì„±í•˜ë©´ í†µì‹ ì´ ê°€ëŠ¥í•˜ë‹¤. âœ .kube kubectl get nodes NAME STATUS ROLES AGE VERSION ip-192-168-1-170.ap-northeast-2.compute.internal Ready &lt;none&gt; 136m v1.31.4-eks-aeac579 ip-192-168-2-244.ap-northeast-2.compute.internal Ready &lt;none&gt; 136m v1.31.4-eks-aeac579 . | . private . | EKS í´ëŸ¬ìŠ¤í„°ì˜ API ì—”ë“œí¬ì¸íŠ¸ê°€ ì‚¬ì„¤ IP(í”„ë¼ì´ë¹— IP)ë§Œ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •ë¨ | EKSê°€ ê´€ë¦¬í•˜ëŠ” ENI(Elastic Network Interface)ì— ì‚¬ì„¤ IPê°€ í• ë‹¹ë˜ì–´, í•´ë‹¹ EKS Control Planeì´ ìœ„ì¹˜í•œ ì„œë¸Œë„· ë‚´ì—ì„œë§Œ ì ‘ê·¼ì´ ê°€ëŠ¥ | VPC ë‚´ë¶€ë‚˜ í•´ë‹¹ VPCì™€ ì—°ê²°ëœ ë„¤íŠ¸ì›Œí¬(ì˜ˆ: VPN, Direct Connect)ì—ì„œë§Œ API ì„œë²„ë¡œ í†µì‹ í•  ìˆ˜ ìˆë‹¤. | . aws eks update-cluster-config \\ --region $AWS_DEFAULT_REGION \\ --name $CLUSTER_NAME \\ --resources-vpc-config endpointPublicAccess=false,endpointPrivateAccess=true { \"update\": { \"id\": \"b7526d2d-3009-3f5e-ab64-2c255053dcf4\", \"status\": \"InProgress\", \"type\": \"EndpointAccessUpdate\", \"params\": [ { \"type\": \"EndpointPublicAccess\", \"value\": \"false\" }, { \"type\": \"EndpointPrivateAccess\", \"value\": \"true\" }, { \"type\": \"PublicAccessCidrs\", \"value\": \"[\\\"13.125.166.72/32\\\"]\" } ], \"createdAt\": \"2025-02-09T02:03:37.283000+09:00\", \"errors\": [] } } . ë‹¹ì—°í•˜ê²Œë„ public &amp; private ëª¨ë“œì—ì„œ ì„¤ì •í–ˆë˜ ë¡œì»¬ ì‘ì—…Pcì—ì„œì˜ kubectl í†µì‹ ì€ ë¶ˆê°€í•˜ë‹¤. (proxy serverëŠ” ì‚¬ì„¤ì•„ì´í”¼ ëŒ€ì—­ìœ¼ë¡œ ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì œê³µí•˜ê¸° ë•Œë¬¸) . # kubectl get nodes Unable to connect to the server: dial tcp 192.168.2.82:443: i/o timeout . ",
    "url": "/docs/eks-hands-on/part01/eks/#aws-api-%EC%84%9C%EB%B2%84-%EC%97%94%EB%93%9C%ED%8F%AC%EC%9D%B8%ED%8A%B8-%EC%97%91%EC%84%B8%EC%8A%A4",
    
    "relUrl": "/docs/eks-hands-on/part01/eks/#aws-api-ì„œë²„-ì—”ë“œí¬ì¸íŠ¸-ì—‘ì„¸ìŠ¤"
  },"2": {
    "doc": "eks ì„¤ì¹˜, public & privaet ëª¨ë“œ",
    "title": "eks ì„¤ì¹˜, public & privaet ëª¨ë“œ",
    "content": "cloudformation ìœ¼ë¡œ aws êµ¬ì¡° ì„¤ì¹˜ . aws cloudformation deploy --template-file ./myeks.yaml \\ --stack-name myeks --parameter-overrides KeyName=container SgIngressSshCidr=$(curl -s ipinfo.io/ip)/32 --region ap-northeast-2 ## ssh ì ‘ì† ssh root@$(aws cloudformation describe-stacks --stack-name myeks --query 'Stacks[*].Outputs[0].OutputValue' --output text) root@@X.Y.Z.A's password: qwe123 . ë² ìŠ¤ì²œ ìê²©ì¦ëª… í™•ì¸ . [root@myeks-host ~]# aws configure AWS Access Key ID [None]: A************* AWS Secret Access Key [None]: wiN***zxq4/****************** Default region name [None]: ap-northeast-2 Default output format [None]: [root@myeks-host ~]# [root@myeks-host ~]# aws sts get-caller-identity { \"UserId\": \"A*************\", \"Account\": \"73*********\", \"Arn\": \"arn:aws:iam::73*********\",:user/Administrator\" } . | ê¸°ë³¸ ì •ë³´ í™•ì¸ | . # (ì˜µì…˜) cloud-init ì‹¤í–‰ ê³¼ì • ë¡œê·¸ í™•ì¸ tail -f /var/log/cloud-init-output.log # ì‚¬ìš©ì í™•ì¸ whoami # ê¸°ë³¸ íˆ´ ë° SSH í‚¤ ì„¤ì¹˜ ë“± í™•ì¸ **kubectl version --client=true -o yaml** **eksctl version** **aws --version** **ls /root/.ssh/id_rsa*** # ë„ì»¤ ì—”ì§„ ì„¤ì¹˜ í™•ì¸ **docker info** . | IAM User ìê²© ì¦ëª… ì„¤ì • ë° VPC í™•ì¸ ë° ë³€ìˆ˜ ì§€ì • | . # EKS ë°°í¬í•  VPC ì •ë³´ í™•ì¸ **aws ec2 describe-vpcs --filters \"Name=tag:Name,Values=$CLUSTER_NAME-VPC\" | jq** aws ec2 describe-vpcs --filters \"Name=tag:Name,Values=$CLUSTER_NAME-VPC\" | jq Vpcs[] aws ec2 describe-vpcs --filters \"Name=tag:Name,Values=$CLUSTER_NAME-VPC\" | jq Vpcs[].VpcId aws ec2 describe-vpcs --filters \"Name=tag:Name,Values=$CLUSTER_NAME-VPC\" | jq -r .Vpcs[].VpcId **export VPCID=$(aws ec2 describe-vpcs --filters \"Name=tag:Name,Values=$CLUSTER_NAME-VPC\" | jq -r .Vpcs[].VpcId) echo \"export VPCID=$VPCID\" &gt;&gt; /etc/profile** echo $VPCID ****# EKS ë°°í¬í•  VPCì— ì†í•œ Subnet ì •ë³´ í™•ì¸ aws ec2 describe-subnets --filters \"Name=vpc-id,Values=$VPCID\" --output json | jq aws ec2 describe-subnets --filters \"Name=vpc-id,Values=$VPCID\" --output yaml ****## í¼ë¸”ë¦­ ì„œë¸Œë„· ID í™•ì¸ **aws ec2 describe-subnets --filters Name=tag:Name,Values=\"$CLUSTER_NAME-PublicSubnet1\" | jq** aws ec2 describe-subnets --filters Name=tag:Name,Values=\"$CLUSTER_NAME-PublicSubnet1\" --query \"Subnets[0].[SubnetId]\" --output text **export PubSubnet1=$(aws ec2 describe-subnets --filters Name=tag:Name,Values=\"$CLUSTER_NAME-PublicSubnet1\" --query \"Subnets[0].[SubnetId]\" --output text) export PubSubnet2=$(aws ec2 describe-subnets --filters Name=tag:Name,Values=\"$CLUSTER_NAME-PublicSubnet2\" --query \"Subnets[0].[SubnetId]\" --output text) echo \"export PubSubnet1=$PubSubnet1\" &gt;&gt; /etc/profile echo \"export PubSubnet2=$PubSubnet2\" &gt;&gt; /etc/profile** echo $PubSubnet1 echo $PubSubnet2 . ê´€ë¦¬í˜• ë…¸ë“œ ì„¤ì¹˜ . ì»¨íŠ¸ë¡¤í”Œë ˆì¸ ì„¤ì¹˜ . # ë³€ìˆ˜ í™•ì¸*** echo $AWS_DEFAULT_REGION echo $CLUSTER_NAME echo $VPCID echo $PubSubnet1,$PubSubnet2 # ì˜µì…˜ [í„°ë¯¸ë„1] EC2 ìƒì„± ëª¨ë‹ˆí„°ë§ **while true; do aws ec2 describe-instances --query \"Reservations[*].Instances[*].{PublicIPAdd:PublicIpAddress,PrivateIPAdd:PrivateIpAddress,InstanceName:Tags[?Key=='Name']|[0].Value,Status:State.Name}\" --filters Name=instance-state-name,Values=running --output text ; echo \"------------------------------\" ; sleep 1; done** aws ec2 describe-instances --query \"Reservations[*].Instances[*].{PublicIPAdd:PublicIpAddress,PrivateIPAdd:PrivateIpAddress,InstanceName:Tags[?Key=='Name']|[0].Value,Status:State.Name}\" --filters Name=instance-state-name,Values=running --output table # eks í´ëŸ¬ìŠ¤í„° &amp; ê´€ë¦¬í˜•ë…¸ë“œê·¸ë£¹ ë°°í¬ ì „ ì •ë³´ í™•ì¸ **eksctl create cluster --name $CLUSTER_NAME --region=$AWS_DEFAULT_REGION --nodegroup-name=$CLUSTER_NAME-nodegroup --node-type=t3.medium \\\\ --node-volume-size=30 --vpc-public-subnets \"$PubSubnet1,$PubSubnet2\" --version 1.31 --ssh-access --external-dns-access --dry-run** | yh ****... **vpc**: autoAllocateIPv6: false cidr: 192.168.0.0/16 clusterEndpoints: privateAccess: false publicAccess: true **id**: vpc-0505d154771a3dfdf manageSharedNodeSecurityGroupRules: true nat: gateway: Disable **subnets**: **public**: ap-northeast-2a: az: ap-northeast-2a cidr: 192.168.1.0/24 id: subnet-0d98bee5a7c0dfcc6 ap-northeast-2c: az: ap-northeast-2c cidr: 192.168.2.0/24 id: subnet-09dc49de8d899aeb7 **** **# eks í´ëŸ¬ìŠ¤í„° &amp; ê´€ë¦¬í˜•ë…¸ë“œê·¸ë£¹ ë°°í¬: ì´ 15ë¶„ ì†Œìš”** **eksctl create cluster --name $CLUSTER_NAME --region=$AWS_DEFAULT_REGION --nodegroup-name=$CLUSTER_NAME-nodegroup --node-type=t3.medium \\\\ --node-volume-size=30 --vpc-public-subnets \"$PubSubnet1,$PubSubnet2\" --version 1.31 --ssh-access --external-dns-access --verbose 4** ... 023-04-23 01:32:22 [â–¶] setting current-context to admin@myeks.ap-northeast-2.eksctl.io 2023-04-23 01:32:22 [âœ”] **saved kubeconfig as \"/root/.kube/config\"** ... ì„¤ì¹˜ ë¡œê·¸í™•ì¸ . ì ‘ì†í™•ì¸ . # ë…¸ë“œ IP í™•ì¸ ë° PrivateIP ë³€ìˆ˜ ì§€ì • aws ec2 describe-instances --query \"Reservations[*].Instances[*].{PublicIPAdd:PublicIpAddress,PrivateIPAdd:PrivateIpAddress,InstanceName:Tags[?Key=='Name']|[0].Value,Status:State.Name}\" --filters Name=instance-state-name,Values=running --output table kubectl get node --label-columns=topology.kubernetes.io/zone kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2a kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2c **N1=$(**kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2a -o jsonpath={.items[0].status.addresses[0].address}) **N2=$**(kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2c -o jsonpath={.items[0].status.addresses[0].address}) ****echo $N1, $N2 echo \"export N1=$N1\" &gt;&gt; /etc/profile echo \"export N2=$N2\" &gt;&gt; /etc/profile # eksctl-host ì—ì„œ ë…¸ë“œì˜IPë‚˜ coredns íŒŒë“œIPë¡œ ping í…ŒìŠ¤íŠ¸ ping &lt;IP&gt; ping -c 1 $N1 ping -c 1 $N2 # ë…¸ë“œ ë³´ì•ˆê·¸ë£¹ ID í™•ì¸ aws ec2 describe-security-groups --filters Name=group-name,Values=*nodegroup* --query \"SecurityGroups[*].[GroupId]\" --output text NGSGID=$(aws ec2 describe-security-groups --filters Name=group-name,Values=*nodegroup* --query \"SecurityGroups[*].[GroupId]\" --output text) echo $NGSGID echo \"export NGSGID=$NGSGID\" &gt;&gt; /etc/profile # ë…¸ë“œ ë³´ì•ˆê·¸ë£¹ì— eksctl-host ì—ì„œ ë…¸ë“œ(íŒŒë“œ)ì— ì ‘ì† ê°€ëŠ¥í•˜ê²Œ ë£°(Rule) ì¶”ê°€ ì„¤ì • aws ec2 authorize-security-group-ingress --group-id $NGSGID --protocol '-1' --cidr **192.168.1.100/32** # eksctl-host ì—ì„œ ë…¸ë“œì˜IPë‚˜ coredns íŒŒë“œIPë¡œ ping í…ŒìŠ¤íŠ¸ ping -c 2 $N1 ping -c 2 $N2 # ì›Œì»¤ ë…¸ë“œ SSH ì ‘ì† **ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ec2-user@$N1 hostname ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ec2-user@$N2 hostname** **ssh ec2-user@$N1 exit** **ssh ec2-user@$N2 exit** . ",
    "url": "/docs/eks-hands-on/part01/eks/",
    
    "relUrl": "/docs/eks-hands-on/part01/eks/"
  },"3": {
    "doc": "ëª©ì°¨",
    "title": "ëª©ì°¨",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"4": {
    "doc": "ëª©ì°¨",
    "title": "ëª©ì°¨",
    "content": "eks . | eks hands on | . ",
    "url": "/",
    
    "relUrl": "/"
  },"5": {
    "doc": "ëª©ì°¨",
    "title": "ë¼ì´ì„ ìŠ¤ ì •ë³´",
    "content": "ë³¸ ê°€ì´ë“œëŠ” CC-BY-4.0 ë¼ì´ì„ ìŠ¤ í•˜ì— ê³µê°œí•©ë‹ˆë‹¤. ",
    "url": "/#%EB%9D%BC%EC%9D%B4%EC%84%A0%EC%8A%A4-%EC%A0%95%EB%B3%B4",
    
    "relUrl": "/#ë¼ì´ì„ ìŠ¤-ì •ë³´"
  },"6": {
    "doc": "eks ì‹œì‘í•˜ê¸°",
    "title": "eks ì‹œì‘í•˜ê¸°",
    "content": "eks hands on ì„ ìœ„í•´ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤. ",
    "url": "/docs/eks-hands-on/part01/",
    
    "relUrl": "/docs/eks-hands-on/part01/"
  },"7": {
    "doc": "eks ì‹¤ìŠµ",
    "title": "eks ì‹¤ìŠµ",
    "content": "eks hands on study. ",
    "url": "/docs/eks-hands-on/",
    
    "relUrl": "/docs/eks-hands-on/"
  },"8": {
    "doc": "localì—ì„œ k8s ì„¤ì¹˜í•˜ê¸°",
    "title": "local &gt; K8s",
    "content": "install . sudo apt-get update sudo apt-get install -y docker.io apt-get update sudo apt-get install -y docker.io apt-get install -y docker.io apt-get install sudo systemctl start docker systemctl enable docker usermod -aG docker ens4 curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64 chmod +x ./kind sudo mv ./kind /usr/local/bin/kind . ì„¤ì •íŒŒì¼ . kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane kubeadmConfigPatches: - | kind: InitConfiguration nodeRegistration: kubeletExtraArgs: system-reserved: memory=1Gi - role: worker kubeadmConfigPatches: - | kind: JoinConfiguration nodeRegistration: kubeletExtraArgs: system-reserved: memory=2Gi - role: worker kubeadmConfigPatches: - | kind: JoinConfiguration nodeRegistration: kubeletExtraArgs: system-reserved: memory=2Gi . ì‹¤í–‰ . root@ens4:~# kind create cluster --config kind-config.yaml Creating cluster \"kind\" ... âœ“ Ensuring node image (kindest/node:v1.27.3) ğŸ–¼ âœ“ Preparing nodes ğŸ“¦ ğŸ“¦ ğŸ“¦ âœ“ Writing configuration ğŸ“œ âœ“ Starting control-plane ğŸ•¹ï¸ âœ“ Installing CNI ğŸ”Œ âœ“ Installing StorageClass ğŸ’¾ âœ“ Joining worker nodes ğŸšœ Set kubectl context to \"kind-kind\" You can now use your cluster with: kubectl cluster-info --context kind-kind Have a nice day! ğŸ‘‹ . í…Œë¼í¼ ì„¤ì¹˜ . # Terraform 1.5.5 ë²„ì „ ë‹¤ìš´ë¡œë“œ (ë²„ì „ì€ í•„ìš”ì— ë”°ë¼ ë³€ê²½) curl -LO https://releases.hashicorp.com/terraform/1.5.5/terraform_1.5.5_linux_amd64.zip # ë‹¤ìš´ë¡œë“œí•œ zip íŒŒì¼ ì••ì¶• í•´ì œ unzip terraform_1.5.5_linux_amd64.zip # terraform ë°”ì´ë„ˆë¦¬ë¥¼ /usr/local/binì— ì´ë™í•˜ì—¬ PATHì— í¬í•¨ sudo mv terraform /usr/local/bin/ . í…Œë¼í¼ ì´ìš©í•´ì„œ ì‹¤í–‰ . root@ens4:~# terraform apply Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # null_resource.kind_cluster will be created + resource \"null_resource\" \"kind_cluster\" { + id = (known after apply) } Plan: 1 to add, 0 to change, 0 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes null_resource.kind_cluster: Creating... null_resource.kind_cluster: Provisioning with 'local-exec'... null_resource.kind_cluster (local-exec): Executing: [\"/bin/sh\" \"-c\" \"kind create cluster --config=kind-config.yaml\"] null_resource.kind_cluster (local-exec): Creating cluster \"kind\" ... null_resource.kind_cluster (local-exec): â€¢ Ensuring node image (kindest/node:v1.27.3) ğŸ–¼ ... null_resource.kind_cluster (local-exec): âœ“ Ensuring node image (kindest/node:v1.27.3) ğŸ–¼ null_resource.kind_cluster (local-exec): â€¢ Preparing nodes ğŸ“¦ ğŸ“¦ ğŸ“¦ ... null_resource.kind_cluster (local-exec): âœ“ Preparing nodes ğŸ“¦ ğŸ“¦ ğŸ“¦ null_resource.kind_cluster (local-exec): â€¢ Writing configuration ğŸ“œ ... null_resource.kind_cluster (local-exec): âœ“ Writing configuration ğŸ“œ null_resource.kind_cluster (local-exec): â€¢ Starting control-plane ğŸ•¹ï¸ ... null_resource.kind_cluster: Still creating... [10s elapsed] null_resource.kind_cluster (local-exec): âœ“ Starting control-plane ğŸ•¹ï¸ null_resource.kind_cluster (local-exec): â€¢ Installing CNI ğŸ”Œ ... null_resource.kind_cluster (local-exec): âœ“ Installing CNI ğŸ”Œ null_resource.kind_cluster (local-exec): â€¢ Installing StorageClass ğŸ’¾ ... null_resource.kind_cluster (local-exec): âœ“ Installing StorageClass ğŸ’¾ null_resource.kind_cluster (local-exec): â€¢ Joining worker nodes ğŸšœ ... null_resource.kind_cluster: Still creating... [20s elapsed] null_resource.kind_cluster: Still creating... [30s elapsed] null_resource.kind_cluster (local-exec): âœ“ Joining worker nodes ğŸšœ null_resource.kind_cluster (local-exec): Set kubectl context to \"kind-kind\" null_resource.kind_cluster (local-exec): You can now use your cluster with: null_resource.kind_cluster (local-exec): kubectl cluster-info --context kind-kind null_resource.kind_cluster (local-exec): Have a nice day! ğŸ‘‹ null_resource.kind_cluster: Creation complete after 37s [id=5176424687883728826] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. k8s ì—”ë“œí¬ì¸íŠ¸ í™•ì¸ . root@ens4:~# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES fb8992a8caba kindest/node:v1.27.3 \"/usr/local/bin/entrâ€¦\" 10 minutes ago Up 10 minutes kind-worker c9af0e2ee5a2 kindest/node:v1.27.3 \"/usr/local/bin/entrâ€¦\" 10 minutes ago Up 10 minutes 127.0.0.1:38707-&gt;6443/tcp kind-control-plane 03c2b1076001 kindest/node:v1.27.3 \"/usr/local/bin/entrâ€¦\" 10 minutes ago Up 10 minutes kind-worker2 # ì§ì ‘í™•ì¸ docker exec -it c9af0e2ee5a2 bash root@kind-control-plane:/# curl -k https://localhost:6443/version { \"major\": \"1\", \"minor\": \"27\", \"gitVersion\": \"v1.27.3\", \"gitCommit\": \"25b4e43193bcda6c7328a6d147b1fb73a33f1598\", \"gitTreeState\": \"clean\", \"buildDate\": \"2023-06-15T00:36:28Z\", \"goVersion\": \"go1.20.5\", \"compiler\": \"gc\", \"platform\": \"linux/amd64\" } # í¬íŠ¸ë¡œ í™•ì¸ root@ens4:~/.kube# cat config | grep server server: https://127.0.0.1:38707 root@ens4:~/.kube# curl -k https://127.0.0.1:38707/version { \"major\": \"1\", \"minor\": \"27\", \"gitVersion\": \"v1.27.3\", \"gitCommit\": \"25b4e43193bcda6c7328a6d147b1fb73a33f1598\", \"gitTreeState\": \"clean\", \"buildDate\": \"2023-06-15T00:36:28Z\", \"goVersion\": \"go1.20.5\", \"compiler\": \"gc\", \"platform\": \"linux/amd64\" } . ",
    "url": "/docs/eks-hands-on/part01/local-k8s/#local--k8s",
    
    "relUrl": "/docs/eks-hands-on/part01/local-k8s/#local--k8s"
  },"9": {
    "doc": "localì—ì„œ k8s ì„¤ì¹˜í•˜ê¸°",
    "title": "ê¸°ë³¸ ì‚¬ìš©",
    "content": "ì„ ì–¸í˜• ë©±ë“±ì„± ì•Œì•„ë³´ê¸° ì‹¤ìŠµ . # í„°ë¯¸ë„1 (ëª¨ë‹ˆí„°ë§) watch -d 'kubectl get pod' # í„°ë¯¸ë„2 # Deployment ë°°í¬(Pod 3ê°œ) kubectl create deployment my-webs --image=gcr.io/google-samples/kubernetes-bootcamp:v1 --replicas=3 kubectl get pod -w # íŒŒë“œ ì¦ê°€ ë° ê°ì†Œ kubectl scale deployment my-webs --replicas=6 &amp;&amp; kubectl get pod -w kubectl scale deployment my-webs --replicas=3 kubectl get pod # ê°•ì œë¡œ íŒŒë“œ ì‚­ì œ : ë°”ë¼ëŠ”ìƒíƒœ + ì„ ì–¸í˜•ì— ëŒ€í•œ ëŒ€ëµì ì¸ í™•ì¸! â‡’ ì–´ë–¤ ì¼ì´ ë²Œì–´ì§€ëŠ”ê°€? kubectl delete pod --all &amp;&amp; kubectl get pod -w kubectl get pod # ì‹¤ìŠµ ì™„ë£Œ í›„ Deployment ì‚­ì œ kubectl delete deploy my-webs . Every 2.0s: kubectl get pod kind-control-plane: Sat Feb 8 13:06:39 2025 NAME READY STATUS RESTARTS AGE my-webs-684fdf4675-726kp 1/1 Running 0 47s my-webs-684fdf4675-kpc6k 1/1 Running 0 47s my-webs-684fdf4675-l52qt 1/1 Running 0 14s my-webs-684fdf4675-rq229 1/1 Running 0 14s my-webs-684fdf4675-vd2mw 1/1 Running 0 47s my-webs-684fdf4675-zhrdt 1/1 Running 0 14s . ì„œë¹„ìŠ¤/ë””í”Œë¡œì´ë¨¼íŠ¸(mario ê²Œì„) ë°°í¬ í…ŒìŠ¤íŠ¸ with AWS CLB . # í„°ë¯¸ë„1 (ëª¨ë‹ˆí„°ë§) watch -d 'kubectl get pod,svc' # ìˆ˜í¼ë§ˆë¦¬ì˜¤ ë””í”Œë¡œì´ë¨¼íŠ¸ ë°°í¬ cat &lt;&lt;EOT &gt; mario.yaml apiVersion: apps/v1 kind: Deployment metadata: name: mario labels: app: mario spec: replicas: 1 selector: matchLabels: app: mario template: metadata: labels: app: mario spec: containers: - name: mario image: pengbai/docker-supermario --- apiVersion: v1 kind: Service metadata: name: mario spec: selector: app: mario ports: - port: 80 protocol: TCP targetPort: 8080 type: NodePort EOT kubectl apply -f mario.yaml kubectl get deploy,svc,ep mario . ",
    "url": "/docs/eks-hands-on/part01/local-k8s/#%EA%B8%B0%EB%B3%B8-%EC%82%AC%EC%9A%A9",
    
    "relUrl": "/docs/eks-hands-on/part01/local-k8s/#ê¸°ë³¸-ì‚¬ìš©"
  },"10": {
    "doc": "localì—ì„œ k8s ì„¤ì¹˜í•˜ê¸°",
    "title": "localì—ì„œ k8s ì„¤ì¹˜í•˜ê¸°",
    "content": " ",
    "url": "/docs/eks-hands-on/part01/local-k8s/",
    
    "relUrl": "/docs/eks-hands-on/part01/local-k8s/"
  }
}
