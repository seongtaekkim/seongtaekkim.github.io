{"0": {
    "doc": "pod 테스트하기",
    "title": "pod 테스트하기",
    "content": "노드에 배포된 컨테이너 정보 확인 Containerd clients 3종 : ctr, nerdctl, crictl . (Administrator@myeks:N/A) [root@myeks-host ~]# ssh ec2-user@$N1 ctr --version ctr github.com/containerd/containerd 1.7.25 (Administrator@myeks:N/A) [root@myeks-host ~]# ssh ec2-user@$N1 ctr NAME: ctr - __ _____/ /______ / ___/ __/ ___/ / /__/ /_/ / \\___/\\__/_/ containerd CLI # 네임스페이스 확인 ssh ec2-user@$N1 sudo ctr ns list NAME LABELS k8s.io # 컨테이너 리스트 확인 for i in $N1 $N2; do echo \"&gt;&gt; node $i &lt;&lt;\"; ssh ec2-user@$i sudo ctr -n k8s.io container list; echo; done CONTAINER IMAGE RUNTIME 28b6a15c475e32cd8777c1963ba684745573d0b6053f80d2d37add0ae841eb45 602401143452.dkr.ecr-fips.us-east-1.amazonaws.com/eks/pause:3.5 io.containerd.runc.v2 4f266ebcee45b133c527df96499e01ec0c020ea72785eb10ef63b20b5826cf7c 602401143452.dkr.ecr-fips.us-east-1.amazonaws.com/eks/pause:3.5 io.containerd.runc.v2 ... # 컨테이너 이미지 확인 for i in $N1 $N2; do echo \"&gt;&gt; node $i &lt;&lt;\"; ssh ec2-user@$i sudo ctr -n k8s.io image list --quiet; echo; done ... # 태스크 리스트 확인 for i in $N1 $N2; do echo \"&gt;&gt; node $i &lt;&lt;\"; ssh ec2-user@$i sudo ctr -n k8s.io task list; echo; done ... ## 예시) 각 테스크의 PID(3706) 확인 ssh ec2-user@$N1 sudo ps -c 3706 PID CLS PRI TTY STAT TIME COMMAND 3099 TS 19 ? Ssl 0:01 kube-proxy --v=2 --config=/var/lib/kube-proxy-config/config --hostname-override=ip-192-168-1-229.ap-northeast-2.compute.internal . ECR 퍼블릭 Repository 사용 : 퍼블릭 Repo 는 설정 시 us-east-1 를 사용 . # 퍼블릭 ECR 인증 aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws cat /root/.docker/config.json | jq # 퍼블릭 Repo 기본 정보 확인 aws ecr-public describe-registries --region us-east-1 | jq { \"registries\": [ { \"registryId\": \"738612635754\", \"registryArn\": \"arn:aws:ecr-public::738612635754:registry/738612635754\", \"registryUri\": \"\", \"verified\": false, \"aliases\": [] } ] } # 퍼블릭 Repo 생성 NICKNAME=seongtki aws ecr-public create-repository --repository-name $NICKNAME/nginx --region us-east-1 # 생성된 퍼블릭 Repo 확인 aws ecr-public describe-repositories --region us-east-1 | jq REPOURI=$(aws ecr-public describe-repositories --region us-east-1 | jq -r .repositories[].repositoryUri) echo $REPOURI public.ecr.aws/o2n8a7w3/seongtki/nginx # 이미지 태그 docker pull nginx:alpine docker images docker tag nginx:alpine $REPOURI:latest docker images # 이미지 업로드 docker push $REPOURI:latest # 파드 실행 kubectl run mynginx --image $REPOURI kubectl get pod NAME READY STATUS RESTARTS AGE mario-6d8c76fd8d-2mx7k 1/1 Running 0 10m mynginx 0/1 ContainerCreating 0 3s kubectl delete pod mynginx ****# 퍼블릭 이미지 삭제 aws ecr-public batch-delete-image \\ --repository-name $NICKNAME/nginx \\ --image-ids imageTag=latest \\ --region us-east-1 # 퍼블릭 Repo 삭제 aws ecr-public delete-repository --repository-name $NICKNAME/nginx --force --region us-east-1 . 서비스/디플로이먼트(mario 게임) 배포 테스트 with AWS CLB . # 터미널1 (모니터링) watch -d 'kubectl get pod,svc' # 수퍼마리오 디플로이먼트 배포 cat &lt;&lt;EOT &gt; mario.yaml apiVersion: apps/v1 kind: Deployment metadata: name: mario labels: app: mario spec: replicas: 1 selector: matchLabels: app: mario template: metadata: labels: app: mario spec: containers: - name: mario image: pengbai/docker-supermario --- apiVersion: v1 kind: Service metadata: name: mario spec: selector: app: mario ports: - port: 80 protocol: TCP targetPort: 8080 type: LoadBalancer EOT kubectl apply -f mario.yaml # 배포 확인 : CLB 배포 확인 kubectl get deploy,svc,ep mario # 마리오 게임 접속 : CLB 주소로 웹 접속 kubectl get svc mario -o jsonpath={.status.loadBalancer.ingress[0].hostname} | awk '{ print \"Maria URL = http://\"$1 }' (Administrator@myeks:N/A) [root@myeks-host ~]# kubectl get svc mario -o jsonpath={.status.loadBalancer.ingress[0].hostname} | awk '{ print \"Maria URL = http://\"$1 }' Maria URL = http://a3fbbb85ed6df4e0288fd3bf4944cfc3-734390217.ap-northeast-2.elb.amazonaws.com . ",
    "url": "/docs/eks-hands-on/part01/application/",
    
    "relUrl": "/docs/eks-hands-on/part01/application/"
  },"1": {
    "doc": "eks 설치, public & privaet 모드",
    "title": "AWS API 서버 엔드포인트 엑세스",
    "content": ". public . while true; do dig +short $APIDNS ; echo \"------------------------------\" ; date; sleep 1; done ------------------------------ 2025. 02. 09. (일) 01:40:42 KST 15.164.132.1 3.38.114.154 . kubelet과 kube-proxy의 엔드포인트는 공인아이피로 되어있다. (즉, 컨트롤 플레인과 노드 통신(kubelet, kube-proxy)이 일어날 때 항상 공인아이피로 송수신된다는 것.) . while true; do ssh ec2-user@$N1 sudo ss -tnp | egrep 'kubelet|kube-proxy' ; echo ; ssh ec2-user@$N2 sudo ss -tnp | egrep 'kubelet|kube-proxy' ; echo \"------------------------------\" ; date; sleep 1; done 2025. 02. 09. (일) 01:40:59 KST ESTAB 0 0 192.168.1.170:36268 15.164.132.1:443 users:((\"kubelet\",pid=2894,fd=26)) ESTAB 0 0 192.168.1.170:55092 3.38.114.154:443 users:((\"kube-proxy\",pid=3113,fd=9)) ESTAB 0 0 [::ffff:192.168.1.170]:10250 [::ffff:192.168.2.122]:47200 users:((\"kubelet\",pid=2894,fd=12)) ESTAB 0 0 [::ffff:192.168.1.170]:10250 [::ffff:192.168.2.156]:52132 users:((\"kubelet\",pid=2894,fd=20)) ESTAB 0 0 192.168.2.244:34406 3.38.114.154:443 users:((\"kube-proxy\",pid=3114,fd=9)) ESTAB 0 0 192.168.2.244:53726 15.164.132.1:443 users:((\"kubelet\",pid=2892,fd=26)) ESTAB 0 0 [::ffff:192.168.2.244]:10250 [::ffff:192.168.2.156]:37720 users:((\"kubelet\",pid=2892,fd=24)) ESTAB 0 0 [::ffff:192.168.2.244]:10250 [::ffff:192.168.2.122]:48800 users:((\"kubelet\",pid=2892,fd=12)) ------------------------------ . public &amp; private . aws eks update-cluster-config --region $AWS_DEFAULT_REGION --name $CLUSTER_NAME --resources-vpc-config endpointPublicAccess=true,publicAccessCidrs=\"$(curl -s ipinfo.io/ip)/32\",endpointPrivateAccess=true { \"update\": { \"id\": \"7e6c1d1e-0020-34a9-90f7-645628f32752\", \"status\": \"InProgress\", \"type\": \"EndpointAccessUpdate\", \"params\": [ { \"type\": \"EndpointPublicAccess\", \"value\": \"true\" }, { \"type\": \"EndpointPrivateAccess\", \"value\": \"true\" }, { \"type\": \"PublicAccessCidrs\", \"value\": \"[\\\"13.125.166.72/32\\\"]\" } ], \"createdAt\": \"2025-02-09T01:40:33.081000+09:00\", \"errors\": [] } } . while true; do dig +short $APIDNS ; echo \"------------------------------\" ; date; sleep 1; done ------------------------------ 2025. 02. 09. (일) 01:40:42 KST 15.164.132.1 3.38.114.154 . kubelet과 kube-proxy의 엔드포인트는 공인아이피로 되어있다. (즉, 컨트롤 플레인과 노드 통신(kubelet, kube-proxy)이 일어날 때 항상 공인아이피로 송수신된다는 것.) . while true; do ssh ec2-user@$N1 sudo ss -tnp | egrep 'kubelet|kube-proxy' ; echo ; ssh ec2-user@$N2 sudo ss -tnp | egrep 'kubelet|kube-proxy' ; echo \"------------------------------\" ; date; sleep 1; done 2025. 02. 09. (일) 01:40:59 KST ESTAB 0 0 192.168.1.170:36268 15.164.132.1:443 users:((\"kubelet\",pid=2894,fd=26)) ESTAB 0 0 192.168.1.170:55092 3.38.114.154:443 users:((\"kube-proxy\",pid=3113,fd=9)) ESTAB 0 0 [::ffff:192.168.1.170]:10250 [::ffff:192.168.2.122]:47200 users:((\"kubelet\",pid=2894,fd=12)) ESTAB 0 0 [::ffff:192.168.1.170]:10250 [::ffff:192.168.2.156]:52132 users:((\"kubelet\",pid=2894,fd=20)) ESTAB 0 0 192.168.2.244:34406 3.38.114.154:443 users:((\"kube-proxy\",pid=3114,fd=9)) ESTAB 0 0 192.168.2.244:53726 15.164.132.1:443 users:((\"kubelet\",pid=2892,fd=26)) ESTAB 0 0 [::ffff:192.168.2.244]:10250 [::ffff:192.168.2.156]:37720 users:((\"kubelet\",pid=2892,fd=24)) ESTAB 0 0 [::ffff:192.168.2.244]:10250 [::ffff:192.168.2.122]:48800 users:((\"kubelet\",pid=2892,fd=12)) ------------------------------ . 위 결과 public &amp; private 으로 변경되는데, 베스천에서 kubctl 명령어가 더이상 실행되지 않는다. 왜냐하면 EKS owned ENI 형태로 관리되는 ENI의 시큐리티그룹에 베스천에 대한 인바운드 설정을 해놓지 않았기에 통신이 되지 않는다. (아래와 같이 처리하면 됨) . ControlPlaneSecurityGroup을 찾고, 베스천 Ip 인바운드를 Expose한다 . CPSGID=$(aws ec2 describe-security-groups --filters Name=group-name,Values=*ControlPlaneSecurityGroup* --query \"SecurityGroups[*].[GroupId]\" --output text) . aws ec2 authorize-security-group-ingress --group-id $CPSGID --protocol '-1' --cidr 192.168.1.100/32 { \"Return\": true, \"SecurityGroupRules\": [ { \"SecurityGroupRuleId\": \"sgr-0e61ded354b5415b0\", \"GroupId\": \"sg-0e301754ec8ac3212\", \"GroupOwnerId\": \"738612635754\", \"IsEgress\": false, \"IpProtocol\": \"-1\", \"FromPort\": -1, \"ToPort\": -1, \"CidrIpv4\": \"192.168.1.100/32\", \"SecurityGroupRuleArn\": \"arn:aws:ec2:ap-northeast-2:738612635754:security-group-rule/sgr-0e61ded354b5415b0\" } ] } . | dig +short $APIDNS | dig 결과가 컨트롤 플레인에서 관리하는 사설아이피가 조회되기 시작한다. | 이는, 데이터플레인과의 통신이 aws 내부적으로 일어난다는 의미 | . ------------------------------ 2025. 02. 09. (일) 01:56:05 KST 192.168.1.152 192.168.2.82 . | node dns 역시 internal로 변경됨 | . kubectl get node -v=6 I0209 01:58:11.886004 18989 loader.go:395] Config loaded from file: /root/.kube/config I0209 01:58:12.857831 18989 round_trippers.go:553] GET https://4049728AFEB8F9CEA17AA2E19BD71657.gr7.ap-northeast-2.eks.amazonaws.com/api/v1/nodes?limit=500 200 OK in 961 milliseconds NAME STATUS ROLES AGE VERSION ip-192-168-1-170.ap-northeast-2.compute.internal Ready &lt;none&gt; 99m v1.31.4-eks-aeac579 ip-192-168-2-244.ap-northeast-2.compute.internal Ready &lt;none&gt; 99m v1.31.4-eks-aeac579 . | 실제 통신은, 아래와같이 재실행 이후 적용됨을 확인 | . # 모니터링 : tcp peer 정보 변화 확인 while true; do ssh ec2-user@$N1 sudo ss -tnp | egrep 'kubelet|kube-proxy' ; echo ; ssh ec2-user@$N2 sudo ss -tnp | egrep 'kubelet|kube-proxy' ; echo \"------------------------------\" ; date; sleep 1; done # kube-proxy rollout kubectl rollout restart ds/kube-proxy -n kube-system . while true; do ssh ec2-user@$N1 sudo ss -tnp | egrep 'kubelet|kube-proxy' ; echo ; ssh ec2-user@$N2 sudo ss -tnp | egrep 'kubelet|kube-proxy' ; echo \"------------------------------\" ; date; sleep 1; done ESTAB 0 0 192.168.1.170:44346 192.168.2.82:443 users:((\"kubelet\",pid=60428,fd=9)) ESTAB 0 0 192.168.1.170:57082 192.168.1.152:443 users:((\"kube-proxy\",pid=60264,fd=9)) ESTAB 0 0 192.168.2.244:51864 192.168.1.152:443 users:((\"kube-proxy\",pid=59674,fd=9)) ESTAB 0 0 192.168.2.244:54588 192.168.2.82:443 users:((\"kubelet\",pid=59859,fd=12)) . 허용아이피를 삭제하면 ? . | 당연히 kubectl api 통신은 된다. 애초에 데이터플레인 이 있는 서브넷에 속해있기 때문에 EKS owned ENI 을 통해 통신하고 있었기에, 허용아이피와는 무관하다. (private 모드에서 통신 가능) . | 다른 작업공간에서 kubectl 통신이 필요하다면, aws 자격증명 후에 aws eks update-kubeconfig --region ap-northeast-2 --name myeks 와 같이 eks 접근정보를 작성하면 통신이 가능하다. ➜ .kube kubectl get nodes NAME STATUS ROLES AGE VERSION ip-192-168-1-170.ap-northeast-2.compute.internal Ready &lt;none&gt; 136m v1.31.4-eks-aeac579 ip-192-168-2-244.ap-northeast-2.compute.internal Ready &lt;none&gt; 136m v1.31.4-eks-aeac579 . | . private . | EKS 클러스터의 API 엔드포인트가 사설 IP(프라이빗 IP)만 사용하도록 설정됨 | EKS가 관리하는 ENI(Elastic Network Interface)에 사설 IP가 할당되어, 해당 EKS Control Plane이 위치한 서브넷 내에서만 접근이 가능 | VPC 내부나 해당 VPC와 연결된 네트워크(예: VPN, Direct Connect)에서만 API 서버로 통신할 수 있다. | . aws eks update-cluster-config \\ --region $AWS_DEFAULT_REGION \\ --name $CLUSTER_NAME \\ --resources-vpc-config endpointPublicAccess=false,endpointPrivateAccess=true { \"update\": { \"id\": \"b7526d2d-3009-3f5e-ab64-2c255053dcf4\", \"status\": \"InProgress\", \"type\": \"EndpointAccessUpdate\", \"params\": [ { \"type\": \"EndpointPublicAccess\", \"value\": \"false\" }, { \"type\": \"EndpointPrivateAccess\", \"value\": \"true\" }, { \"type\": \"PublicAccessCidrs\", \"value\": \"[\\\"13.125.166.72/32\\\"]\" } ], \"createdAt\": \"2025-02-09T02:03:37.283000+09:00\", \"errors\": [] } } . 당연하게도 public &amp; private 모드에서 설정했던 로컬 작업Pc에서의 kubectl 통신은 불가하다. (proxy server는 사설아이피 대역으로 엔드포인트를 제공하기 때문) . # kubectl get nodes Unable to connect to the server: dial tcp 192.168.2.82:443: i/o timeout . ",
    "url": "/docs/eks-hands-on/part01/eks/#aws-api-%EC%84%9C%EB%B2%84-%EC%97%94%EB%93%9C%ED%8F%AC%EC%9D%B8%ED%8A%B8-%EC%97%91%EC%84%B8%EC%8A%A4",
    
    "relUrl": "/docs/eks-hands-on/part01/eks/#aws-api-서버-엔드포인트-엑세스"
  },"2": {
    "doc": "eks 설치, public & privaet 모드",
    "title": "eks 설치, public & privaet 모드",
    "content": "cloudformation 으로 aws 구조 설치 . aws cloudformation deploy --template-file ./myeks.yaml \\ --stack-name myeks --parameter-overrides KeyName=container SgIngressSshCidr=$(curl -s ipinfo.io/ip)/32 --region ap-northeast-2 ## ssh 접속 ssh root@$(aws cloudformation describe-stacks --stack-name myeks --query 'Stacks[*].Outputs[0].OutputValue' --output text) root@@X.Y.Z.A's password: qwe123 . 베스천 자격증명 확인 . [root@myeks-host ~]# aws configure AWS Access Key ID [None]: A************* AWS Secret Access Key [None]: wiN***zxq4/****************** Default region name [None]: ap-northeast-2 Default output format [None]: [root@myeks-host ~]# [root@myeks-host ~]# aws sts get-caller-identity { \"UserId\": \"A*************\", \"Account\": \"73*********\", \"Arn\": \"arn:aws:iam::73*********\",:user/Administrator\" } . | 기본 정보 확인 | . # (옵션) cloud-init 실행 과정 로그 확인 tail -f /var/log/cloud-init-output.log # 사용자 확인 whoami # 기본 툴 및 SSH 키 설치 등 확인 **kubectl version --client=true -o yaml** **eksctl version** **aws --version** **ls /root/.ssh/id_rsa*** # 도커 엔진 설치 확인 **docker info** . | IAM User 자격 증명 설정 및 VPC 확인 및 변수 지정 | . # EKS 배포할 VPC 정보 확인 **aws ec2 describe-vpcs --filters \"Name=tag:Name,Values=$CLUSTER_NAME-VPC\" | jq** aws ec2 describe-vpcs --filters \"Name=tag:Name,Values=$CLUSTER_NAME-VPC\" | jq Vpcs[] aws ec2 describe-vpcs --filters \"Name=tag:Name,Values=$CLUSTER_NAME-VPC\" | jq Vpcs[].VpcId aws ec2 describe-vpcs --filters \"Name=tag:Name,Values=$CLUSTER_NAME-VPC\" | jq -r .Vpcs[].VpcId **export VPCID=$(aws ec2 describe-vpcs --filters \"Name=tag:Name,Values=$CLUSTER_NAME-VPC\" | jq -r .Vpcs[].VpcId) echo \"export VPCID=$VPCID\" &gt;&gt; /etc/profile** echo $VPCID ****# EKS 배포할 VPC에 속한 Subnet 정보 확인 aws ec2 describe-subnets --filters \"Name=vpc-id,Values=$VPCID\" --output json | jq aws ec2 describe-subnets --filters \"Name=vpc-id,Values=$VPCID\" --output yaml ****## 퍼블릭 서브넷 ID 확인 **aws ec2 describe-subnets --filters Name=tag:Name,Values=\"$CLUSTER_NAME-PublicSubnet1\" | jq** aws ec2 describe-subnets --filters Name=tag:Name,Values=\"$CLUSTER_NAME-PublicSubnet1\" --query \"Subnets[0].[SubnetId]\" --output text **export PubSubnet1=$(aws ec2 describe-subnets --filters Name=tag:Name,Values=\"$CLUSTER_NAME-PublicSubnet1\" --query \"Subnets[0].[SubnetId]\" --output text) export PubSubnet2=$(aws ec2 describe-subnets --filters Name=tag:Name,Values=\"$CLUSTER_NAME-PublicSubnet2\" --query \"Subnets[0].[SubnetId]\" --output text) echo \"export PubSubnet1=$PubSubnet1\" &gt;&gt; /etc/profile echo \"export PubSubnet2=$PubSubnet2\" &gt;&gt; /etc/profile** echo $PubSubnet1 echo $PubSubnet2 . 관리형 노드 설치 . 컨트롤플레인 설치 . # 변수 확인*** echo $AWS_DEFAULT_REGION echo $CLUSTER_NAME echo $VPCID echo $PubSubnet1,$PubSubnet2 # 옵션 [터미널1] EC2 생성 모니터링 **while true; do aws ec2 describe-instances --query \"Reservations[*].Instances[*].{PublicIPAdd:PublicIpAddress,PrivateIPAdd:PrivateIpAddress,InstanceName:Tags[?Key=='Name']|[0].Value,Status:State.Name}\" --filters Name=instance-state-name,Values=running --output text ; echo \"------------------------------\" ; sleep 1; done** aws ec2 describe-instances --query \"Reservations[*].Instances[*].{PublicIPAdd:PublicIpAddress,PrivateIPAdd:PrivateIpAddress,InstanceName:Tags[?Key=='Name']|[0].Value,Status:State.Name}\" --filters Name=instance-state-name,Values=running --output table # eks 클러스터 &amp; 관리형노드그룹 배포 전 정보 확인 **eksctl create cluster --name $CLUSTER_NAME --region=$AWS_DEFAULT_REGION --nodegroup-name=$CLUSTER_NAME-nodegroup --node-type=t3.medium \\\\ --node-volume-size=30 --vpc-public-subnets \"$PubSubnet1,$PubSubnet2\" --version 1.31 --ssh-access --external-dns-access --dry-run** | yh ****... **vpc**: autoAllocateIPv6: false cidr: 192.168.0.0/16 clusterEndpoints: privateAccess: false publicAccess: true **id**: vpc-0505d154771a3dfdf manageSharedNodeSecurityGroupRules: true nat: gateway: Disable **subnets**: **public**: ap-northeast-2a: az: ap-northeast-2a cidr: 192.168.1.0/24 id: subnet-0d98bee5a7c0dfcc6 ap-northeast-2c: az: ap-northeast-2c cidr: 192.168.2.0/24 id: subnet-09dc49de8d899aeb7 **** **# eks 클러스터 &amp; 관리형노드그룹 배포: 총 15분 소요** **eksctl create cluster --name $CLUSTER_NAME --region=$AWS_DEFAULT_REGION --nodegroup-name=$CLUSTER_NAME-nodegroup --node-type=t3.medium \\\\ --node-volume-size=30 --vpc-public-subnets \"$PubSubnet1,$PubSubnet2\" --version 1.31 --ssh-access --external-dns-access --verbose 4** ... 023-04-23 01:32:22 [▶] setting current-context to admin@myeks.ap-northeast-2.eksctl.io 2023-04-23 01:32:22 [✔] **saved kubeconfig as \"/root/.kube/config\"** ... 설치 로그확인 . 접속확인 . # 노드 IP 확인 및 PrivateIP 변수 지정 aws ec2 describe-instances --query \"Reservations[*].Instances[*].{PublicIPAdd:PublicIpAddress,PrivateIPAdd:PrivateIpAddress,InstanceName:Tags[?Key=='Name']|[0].Value,Status:State.Name}\" --filters Name=instance-state-name,Values=running --output table kubectl get node --label-columns=topology.kubernetes.io/zone kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2a kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2c **N1=$(**kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2a -o jsonpath={.items[0].status.addresses[0].address}) **N2=$**(kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2c -o jsonpath={.items[0].status.addresses[0].address}) ****echo $N1, $N2 echo \"export N1=$N1\" &gt;&gt; /etc/profile echo \"export N2=$N2\" &gt;&gt; /etc/profile # eksctl-host 에서 노드의IP나 coredns 파드IP로 ping 테스트 ping &lt;IP&gt; ping -c 1 $N1 ping -c 1 $N2 # 노드 보안그룹 ID 확인 aws ec2 describe-security-groups --filters Name=group-name,Values=*nodegroup* --query \"SecurityGroups[*].[GroupId]\" --output text NGSGID=$(aws ec2 describe-security-groups --filters Name=group-name,Values=*nodegroup* --query \"SecurityGroups[*].[GroupId]\" --output text) echo $NGSGID echo \"export NGSGID=$NGSGID\" &gt;&gt; /etc/profile # 노드 보안그룹에 eksctl-host 에서 노드(파드)에 접속 가능하게 룰(Rule) 추가 설정 aws ec2 authorize-security-group-ingress --group-id $NGSGID --protocol '-1' --cidr **192.168.1.100/32** # eksctl-host 에서 노드의IP나 coredns 파드IP로 ping 테스트 ping -c 2 $N1 ping -c 2 $N2 # 워커 노드 SSH 접속 **ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ec2-user@$N1 hostname ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ec2-user@$N2 hostname** **ssh ec2-user@$N1 exit** **ssh ec2-user@$N2 exit** . ",
    "url": "/docs/eks-hands-on/part01/eks/",
    
    "relUrl": "/docs/eks-hands-on/part01/eks/"
  },"3": {
    "doc": "목차",
    "title": "목차",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"4": {
    "doc": "목차",
    "title": "목차",
    "content": "eks . | eks hands on | . ",
    "url": "/",
    
    "relUrl": "/"
  },"5": {
    "doc": "목차",
    "title": "라이선스 정보",
    "content": "본 가이드는 CC-BY-4.0 라이선스 하에 공개합니다. ",
    "url": "/#%EB%9D%BC%EC%9D%B4%EC%84%A0%EC%8A%A4-%EC%A0%95%EB%B3%B4",
    
    "relUrl": "/#라이선스-정보"
  },"6": {
    "doc": "eks 시작하기",
    "title": "eks 시작하기",
    "content": "eks hands on 을 위해 기본 테스트를 진행합니다. ",
    "url": "/docs/eks-hands-on/part01/",
    
    "relUrl": "/docs/eks-hands-on/part01/"
  },"7": {
    "doc": "eks 실습",
    "title": "eks 실습",
    "content": "eks hands on study. ",
    "url": "/docs/eks-hands-on/",
    
    "relUrl": "/docs/eks-hands-on/"
  },"8": {
    "doc": "local에서 k8s 설치하기",
    "title": "local &gt; K8s",
    "content": "install . sudo apt-get update sudo apt-get install -y docker.io apt-get update sudo apt-get install -y docker.io apt-get install -y docker.io apt-get install sudo systemctl start docker systemctl enable docker usermod -aG docker ens4 curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64 chmod +x ./kind sudo mv ./kind /usr/local/bin/kind . 설정파일 . kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane kubeadmConfigPatches: - | kind: InitConfiguration nodeRegistration: kubeletExtraArgs: system-reserved: memory=1Gi - role: worker kubeadmConfigPatches: - | kind: JoinConfiguration nodeRegistration: kubeletExtraArgs: system-reserved: memory=2Gi - role: worker kubeadmConfigPatches: - | kind: JoinConfiguration nodeRegistration: kubeletExtraArgs: system-reserved: memory=2Gi . 실행 . root@ens4:~# kind create cluster --config kind-config.yaml Creating cluster \"kind\" ... ✓ Ensuring node image (kindest/node:v1.27.3) 🖼 ✓ Preparing nodes 📦 📦 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 ✓ Joining worker nodes 🚜 Set kubectl context to \"kind-kind\" You can now use your cluster with: kubectl cluster-info --context kind-kind Have a nice day! 👋 . 테라폼 설치 . # Terraform 1.5.5 버전 다운로드 (버전은 필요에 따라 변경) curl -LO https://releases.hashicorp.com/terraform/1.5.5/terraform_1.5.5_linux_amd64.zip # 다운로드한 zip 파일 압축 해제 unzip terraform_1.5.5_linux_amd64.zip # terraform 바이너리를 /usr/local/bin에 이동하여 PATH에 포함 sudo mv terraform /usr/local/bin/ . 테라폼 이용해서 실행 . root@ens4:~# terraform apply Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # null_resource.kind_cluster will be created + resource \"null_resource\" \"kind_cluster\" { + id = (known after apply) } Plan: 1 to add, 0 to change, 0 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes null_resource.kind_cluster: Creating... null_resource.kind_cluster: Provisioning with 'local-exec'... null_resource.kind_cluster (local-exec): Executing: [\"/bin/sh\" \"-c\" \"kind create cluster --config=kind-config.yaml\"] null_resource.kind_cluster (local-exec): Creating cluster \"kind\" ... null_resource.kind_cluster (local-exec): • Ensuring node image (kindest/node:v1.27.3) 🖼 ... null_resource.kind_cluster (local-exec): ✓ Ensuring node image (kindest/node:v1.27.3) 🖼 null_resource.kind_cluster (local-exec): • Preparing nodes 📦 📦 📦 ... null_resource.kind_cluster (local-exec): ✓ Preparing nodes 📦 📦 📦 null_resource.kind_cluster (local-exec): • Writing configuration 📜 ... null_resource.kind_cluster (local-exec): ✓ Writing configuration 📜 null_resource.kind_cluster (local-exec): • Starting control-plane 🕹️ ... null_resource.kind_cluster: Still creating... [10s elapsed] null_resource.kind_cluster (local-exec): ✓ Starting control-plane 🕹️ null_resource.kind_cluster (local-exec): • Installing CNI 🔌 ... null_resource.kind_cluster (local-exec): ✓ Installing CNI 🔌 null_resource.kind_cluster (local-exec): • Installing StorageClass 💾 ... null_resource.kind_cluster (local-exec): ✓ Installing StorageClass 💾 null_resource.kind_cluster (local-exec): • Joining worker nodes 🚜 ... null_resource.kind_cluster: Still creating... [20s elapsed] null_resource.kind_cluster: Still creating... [30s elapsed] null_resource.kind_cluster (local-exec): ✓ Joining worker nodes 🚜 null_resource.kind_cluster (local-exec): Set kubectl context to \"kind-kind\" null_resource.kind_cluster (local-exec): You can now use your cluster with: null_resource.kind_cluster (local-exec): kubectl cluster-info --context kind-kind null_resource.kind_cluster (local-exec): Have a nice day! 👋 null_resource.kind_cluster: Creation complete after 37s [id=5176424687883728826] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. k8s 엔드포인트 확인 . root@ens4:~# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES fb8992a8caba kindest/node:v1.27.3 \"/usr/local/bin/entr…\" 10 minutes ago Up 10 minutes kind-worker c9af0e2ee5a2 kindest/node:v1.27.3 \"/usr/local/bin/entr…\" 10 minutes ago Up 10 minutes 127.0.0.1:38707-&gt;6443/tcp kind-control-plane 03c2b1076001 kindest/node:v1.27.3 \"/usr/local/bin/entr…\" 10 minutes ago Up 10 minutes kind-worker2 # 직접확인 docker exec -it c9af0e2ee5a2 bash root@kind-control-plane:/# curl -k https://localhost:6443/version { \"major\": \"1\", \"minor\": \"27\", \"gitVersion\": \"v1.27.3\", \"gitCommit\": \"25b4e43193bcda6c7328a6d147b1fb73a33f1598\", \"gitTreeState\": \"clean\", \"buildDate\": \"2023-06-15T00:36:28Z\", \"goVersion\": \"go1.20.5\", \"compiler\": \"gc\", \"platform\": \"linux/amd64\" } # 포트로 확인 root@ens4:~/.kube# cat config | grep server server: https://127.0.0.1:38707 root@ens4:~/.kube# curl -k https://127.0.0.1:38707/version { \"major\": \"1\", \"minor\": \"27\", \"gitVersion\": \"v1.27.3\", \"gitCommit\": \"25b4e43193bcda6c7328a6d147b1fb73a33f1598\", \"gitTreeState\": \"clean\", \"buildDate\": \"2023-06-15T00:36:28Z\", \"goVersion\": \"go1.20.5\", \"compiler\": \"gc\", \"platform\": \"linux/amd64\" } . ",
    "url": "/docs/eks-hands-on/part01/local-k8s/#local--k8s",
    
    "relUrl": "/docs/eks-hands-on/part01/local-k8s/#local--k8s"
  },"9": {
    "doc": "local에서 k8s 설치하기",
    "title": "기본 사용",
    "content": "선언형 멱등성 알아보기 실습 . # 터미널1 (모니터링) watch -d 'kubectl get pod' # 터미널2 # Deployment 배포(Pod 3개) kubectl create deployment my-webs --image=gcr.io/google-samples/kubernetes-bootcamp:v1 --replicas=3 kubectl get pod -w # 파드 증가 및 감소 kubectl scale deployment my-webs --replicas=6 &amp;&amp; kubectl get pod -w kubectl scale deployment my-webs --replicas=3 kubectl get pod # 강제로 파드 삭제 : 바라는상태 + 선언형에 대한 대략적인 확인! ⇒ 어떤 일이 벌어지는가? kubectl delete pod --all &amp;&amp; kubectl get pod -w kubectl get pod # 실습 완료 후 Deployment 삭제 kubectl delete deploy my-webs . Every 2.0s: kubectl get pod kind-control-plane: Sat Feb 8 13:06:39 2025 NAME READY STATUS RESTARTS AGE my-webs-684fdf4675-726kp 1/1 Running 0 47s my-webs-684fdf4675-kpc6k 1/1 Running 0 47s my-webs-684fdf4675-l52qt 1/1 Running 0 14s my-webs-684fdf4675-rq229 1/1 Running 0 14s my-webs-684fdf4675-vd2mw 1/1 Running 0 47s my-webs-684fdf4675-zhrdt 1/1 Running 0 14s . 서비스/디플로이먼트(mario 게임) 배포 테스트 with AWS CLB . # 터미널1 (모니터링) watch -d 'kubectl get pod,svc' # 수퍼마리오 디플로이먼트 배포 cat &lt;&lt;EOT &gt; mario.yaml apiVersion: apps/v1 kind: Deployment metadata: name: mario labels: app: mario spec: replicas: 1 selector: matchLabels: app: mario template: metadata: labels: app: mario spec: containers: - name: mario image: pengbai/docker-supermario --- apiVersion: v1 kind: Service metadata: name: mario spec: selector: app: mario ports: - port: 80 protocol: TCP targetPort: 8080 type: NodePort EOT kubectl apply -f mario.yaml kubectl get deploy,svc,ep mario . ",
    "url": "/docs/eks-hands-on/part01/local-k8s/#%EA%B8%B0%EB%B3%B8-%EC%82%AC%EC%9A%A9",
    
    "relUrl": "/docs/eks-hands-on/part01/local-k8s/#기본-사용"
  },"10": {
    "doc": "local에서 k8s 설치하기",
    "title": "local에서 k8s 설치하기",
    "content": " ",
    "url": "/docs/eks-hands-on/part01/local-k8s/",
    
    "relUrl": "/docs/eks-hands-on/part01/local-k8s/"
  }
}
